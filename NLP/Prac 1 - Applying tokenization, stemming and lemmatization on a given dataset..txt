#pip cmd :
#pip install nltk
#python -m pip install spacy
#python -m spacy download en_core_web_sm
#pip install scikit-learn


Practical 1 : 
Applying Tokenization, Stemming and Lemmatization on a given dataset.

import nltk
import nltk.corpus
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import PorterStemmer
from nltk.probability import FreqDist
from nltk.stem import WordNetLemmatizer

sample="""Natural language processing (NLP) refers to the branch of computer science—and more specifically,the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can."""



#Apply Tokenization by passing the sample text to word tokenizer
Sample_Tokens=word_tokenize(sample)
print("Sample Tokens: \n", Sample_Tokens)

#Getting type of the data
print("\nType of Sample Tokens: \n", type(Sample_Tokens))

#Getting length of the data
print("\nLength of Sample Tokens: \n", len(Sample_Tokens))


#Finding frequency of the words coming in the sample text
FDist= FreqDist(Sample_Tokens)
print("\nFrequency Distribution: \n", FDist)

top5=FDist.most_common(5)
print("\nTop 5 Words: \n", top5)

# Stemming
pst=PorterStemmer()
print("\nStemming of Words: ")
print("Giving: ", pst.stem("Giving"))
print("Buying: ", pst.stem("Buying"))
print("Studying: ", pst.stem("Studying"))
print("Going: ", pst.stem("Going"))

# Lemmatization
lmt = WordNetLemmatizer()
print("\nLemmatization of Words: ")
print("Geese: ", lmt.lemmatize("geese"))
print("Cacti: ", lmt.lemmatize("cacti"))

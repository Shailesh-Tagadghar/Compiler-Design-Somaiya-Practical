Practical 1 : 
Applying Tokenization, Stemming and Lemmatization on a given dataset.

import nltk
import nltk.corpus
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import PorterStemmer
from nltk.probability import FreqDist
from nltk.stem import WordNetLemmatizer


sample="""Natural language processing (NLP) refers to the branch of computer science—and more specifically, 
the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text 
and spoken words in much the same way human beings can."""



#Apply Tokenization by passing the sample text to word tokenizer
Sample_Tokens=word_tokenize(sample)
Sample_Tokens

#Getting type of the data
type(Sample_Tokens)

#Getting length of the data
len(Sample_Tokens)

#Finding frequency of the words coming in the sample text
FDist= FreqDist(Sample_Tokens)
FDist

top5=FDist.most_common(5)
top5

# Stemming
pst=PorterStemmer()
pst.stem("Giving")
pst.stem("Buying")
pst.stem("Studying")

# Lemmatization
lemmatizer= WordNetLemmatizer()
lemmatizer.lemmatize("goose")
